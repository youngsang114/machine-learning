{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 엔트로피\n",
    "    - 잘 섞여있고, 펴져있는 정도\n",
    "    - 엔트로피가 낮아야지 분류할때 좋다\n",
    "    - -∑ (pi^2 * log2(pi))\n",
    "\n",
    "#### 정보이득(information gain)\n",
    "    - 정보 획득이 크면 분류할때 좋다\n",
    "    - 분기 이전의 불순도와 분기 이후의 불순도의 차이를 정보 획득이라고 한다\n",
    "    - 전 엔트로피 - 현재 엔트로피\n",
    "    - 1 - 엔트로피\n",
    "    - information gain이 큰 순서대로 분기를 나눠준다\n",
    "\n",
    "#### 지니 계수\n",
    "    - 불순도라고 한다\n",
    "    - 1 - ∑ p^2\n",
    "    - 최댓값은 0.5\n",
    "\n",
    "- 지니계수가 낮을수록, 엔트로피가 낮을수록, 정보이득이 많을 수록 분류할때 유리하다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "    - 노드\n",
    "        - 루트 노드 : 시작 노드\n",
    "        - 규칙 노드 : 중간 노드\n",
    "        - 리프 노트 : 마지막 노드\n",
    "        - 결정 트리의 기본 아이디어는, Leaf Node가 가장 섞이지 않은 상태로 완전히 분류되는 것, 즉 복잡성(entropy)이 낮도록 만드는 것\n",
    "    - depth: 얼마나 분기하느냐?\n",
    "        - max_depth가 너무 크면 과적합(overfitting)의 문제가 있다\n",
    "    - 가지 치기(pruning)\n",
    "        - 과대 적합(overfitting)을 막기 위해서 가지 치기를 한다\n",
    "    - 장점: \n",
    "        -데이터 전처리(정규화,결측치,이상치 제거)를 안해도 된다\n",
    "        - 수치형, 범주형 변수를 한꺼번에 다룰 수 있다\n",
    "        - 쉽다, 직관적이다\n",
    "    - 단점:\n",
    "        - 과적합으로 알고리즘 성능이 떨어진다\n",
    "        - 극복하기 위해 트리의 크기를 사전에 제한하는 튜닝 필요\n",
    "        - 샘플의 사이즈가 커지면 효율성 및 가독성이 떨어진다떨어진다\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Parameter\n",
    "    - max_depth : 트리의 최대 깊이, 층\n",
    "    - max_feautres : 최대 피쳐 개수 선정\n",
    "    - min_samples_split : 노드를 분할하기 위한 최소한의 샘플 데이터 수\n",
    "    - min_samples_leaf : Leaf node가 되기 위한 최소한의 샘플 데이터 수, 데이터의 비대칭이 있을땐 특정 데이터가 극도로 작을 수 있으니 작게 설정 필요\n",
    "    - max_leaf_nodes : Leaf node의 최대 개수\n",
    "    - max_depth, min_samples_leaf를 주로 쓴다\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,\n",
       "       0, 1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33, random_state=42)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dt_clf.fit(X_train, y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
