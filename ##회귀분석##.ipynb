{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀 분석 (Regression)\n",
    "- 여러개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법\n",
    "- 최적의 weight, bias를 결정 (regression coefficient -> 회귀 계수 찾기)\n",
    "-  유형\n",
    "    - 단일 회귀: 독립변수 개수 1개, 선형 회귀 \n",
    "    - 다중 회귀: 독립변수 개수 2개, 비션형 회귀\n",
    "    - 다항 회귀: 여러개의 항이 존재하는 식\n",
    "- 피쳐들은 서로 독립적이어야 한다.\n",
    "- 피처 셀렉션, 익스트렉션, 규제를 이용해서 피쳐간의 다중 공선성 문제를(mullti-collinerity)문제를 해결해야 한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.linear_model.LinearRegression\n",
    "- sklearn.linear_model.LinearRegression\n",
    "class sklearn.linear_model.LinearRegression(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameters: \n",
    "    - fit_intercept\n",
    "        - 논리형(bool). default = True\n",
    "        - 사용할 모형에 절편을 추가할지 말지 결정한다.\n",
    "    - normalize : \n",
    "        - 논리형(bool). default = False\n",
    "        - 인자에 True 를 입력 시 추후 fitting 하는 X의 관측들에 대해 각 변수마다 평균값을 빼고 L2 norm으로 나누어 정규화를 실행한다\n",
    "        - fit_intercept 옵션이 False라면 작동하지 않는다. \n",
    "- attributes:\n",
    "    - coef_ : \n",
    "        - numpy 의 array 형 반환    \n",
    "        - fitting 된 모형의 계수 추정치 벡터를 넘파이 배열 형태로 반환한다. \n",
    "    - intercept_ :\n",
    "        - float 혹은 array \n",
    "        - fitting 된 모형의 절편 추정치를 실수로 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.svm.SVR(support vector regressor)\n",
    "- class sklearn.svm.SVR(*, kernel='rbf', degree=3, gamma='scale', coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameters: \n",
    "    - kernel\n",
    "        - default=’rbf’ {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} \n",
    "        - 선형 데이터셋의 경우 'linear'를, 비선형 데이터셋은 'poly(다항식)', 'rbf(가우시안)'을 주로 사용\n",
    "    - C\n",
    "        - defualt = 1.0\n",
    "        - 오류를 얼마나 허용할 것인지 (규제항)\n",
    "        - 클수록 하드마진, 작을수록 소프트마진에 가까움\n",
    "    - gamma\n",
    "        - default = 'scale'\n",
    "        - 결정경계를 얼마나 유연하게 그릴지 결정\n",
    "        - 클수록 오버피팅 발생 가능성 높아짐\n",
    "    - degree\n",
    "        - default = 3\n",
    "        - 다항식 커널의 차수 결정\n",
    "    - coef0\n",
    "        - defalut= 0.0\n",
    "        - 다항식 커널에 있는 상수항 r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.linear_model.Lasso\n",
    "- class sklearn.linear_model.Lasso(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameters\n",
    "    - alpha:\n",
    "        - 실수형(float). default = 1.0\n",
    "        - 모형에 가할 L1 규제의 강도를 조절한다. (양의 실수)\n",
    "        - 값이 커질수록 규제가 강해져 모형의 복잡도를 낮추고 (계수를 0에 더 가깝게 만든다.)\n",
    "        - 편향을 증가시키며 분산은 감소시킨다\n",
    "    - max_iter:\n",
    "        - 정수형(int), default = None\n",
    "        - 수행할 최대 반복 수를 결정한다.\n",
    "        - lasso 알고리즘은 좌표하강법(coordinate descent)을 이용하기 때문에 iterative 한 모형이고 때문에 수렴이 오래걸리거나 불가능한 경우를 위해 최대 반복 수를 지정해야한다.\n",
    "    - warm_start : \n",
    "        - 논리형(bool), default = True\n",
    "        - 기존에 fitting된 모형을 재적합할 때, 기존의 학습된 모형의 속성 / 계수 추정값 등을 유지한 새로운 적합의 초기값으로 사용한다.\n",
    "    -  fit_intercept​​ :\n",
    "        - 논리형(bool). default = True\n",
    "        - 사용할 모형에 절편을 추가할지 말지 결정한다.      \n",
    "    - normalize​​ : \n",
    "        - 논리형(bool). default = False\n",
    "        - 인자에 True 를 입력 시 추후 fitting 하는 X의 관측들에 대해 각 변수마다 평균값을 빼고 L2 norm으로 나누어 정규화를 실행한다\n",
    "        - fit_intercept 옵션이 False라면 작동하지 않는다. \n",
    "\n",
    "- attributes\n",
    "    - coef_ : \n",
    "        - numpy 의 array 형 반환    \n",
    "        - fitting 된 모형의 계수 추정치 벡터를 넘파이 배열 형태로 반환한다. \n",
    "    - intercept_ :\n",
    "        - float 혹은 array \n",
    "        - fitting 된 모형의 절편 추정치를 실수로 반환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.linear_model.Ridge\n",
    "- class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, copy_X=True, max_iter=None, tol=0.0001, solver='auto', positive=False, random_state=None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  parameters\n",
    "    - alpha:\n",
    "        - 실수형(float). default = 1.0\n",
    "        - 모형에 가할 L2 규제의 강도를 조절한다. (양의 실수)\n",
    "        - 값이 커질수록 규제가 강해져 모형의 복잡도를 낮추고 (계수를 0에 더 가깝게 만든다.)\n",
    "        - 편향을 증가시키며 분산은 감소시킨다\n",
    "    - solver : \n",
    "        - 문자형(str). default = 'auto'\n",
    "        - 릿지회귀의 회귀계수를 추정하기 위해 사용하는 최적화 방법을 결정한다\n",
    "        - 종류 : svd, cholesky, lsqr, sparse_cg, sag, saga ... \n",
    "    - max_iter : \n",
    "        - 정수형(int), default = None\n",
    "        - 켤레기울기법 / 공역기울기법(conjugate gradient method)을 사용하는 solver에 대해 solving 알고리즘의 최대 반복 수를 설정한다. \n",
    "        -  회귀계수를 반복에 의한 최적 근사치로 결정​하기 때문에 최종 계수 수렴(결정)을 위한 최대 반복수를 지정해 줘야한다.\n",
    "    -  fit_intercept​​ :\n",
    "        - 논리형(bool). default = True\n",
    "        - 사용할 모형에 절편을 추가할지 말지 결정한다.      \n",
    "    - normalize​​ : \n",
    "        - 논리형(bool). default = False\n",
    "        - 인자에 True 를 입력 시 추후 fitting 하는 X의 관측들에 대해 각 변수마다 평균값을 빼고 L2 norm으로 나누어 정규화를 실행한다\n",
    "        - fit_intercept 옵션이 False라면 작동하지 않는다. \n",
    "\n",
    "    - random_state :\n",
    "        - 정수형(int), default = None\n",
    "        - sag, saga 확률적 평균 그레디언트 하강법에 기반한 solver를 사용할 경우 데이터를 무작위로 셔플하는데 이때의 random ness를 고정한다.\n",
    "\n",
    "- attributes\n",
    "    - coef_ : \n",
    "        - numpy 의 array 형 반환    \n",
    "        - fitting 된 모형의 계수 추정치 벡터를 넘파이 배열 형태로 반환한다. \n",
    "    - intercept_ :\n",
    "        - float 혹은 array \n",
    "        - fitting 된 모형의 절편 추정치를 실수로 반환한다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn.linear_model.ElasticNet\n",
    "- class sklearn.linear_model.ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameters\n",
    "    - alpha : \n",
    "        - 실수형(float). default = 1.0\n",
    "        - 모형에 가할 L1, L2 규제의 강도를 복합적으로 조절한다. (양의 실수)\n",
    "        - 값이 커질수록 규제가 강해져 모형의 복잡도를 낮추고 (계수를 0에 더 가깝게 만든다.)\n",
    "        - 편향을 증가시키며 분산은 감소시킨다.\n",
    "    - l1_ratio : \n",
    "        - 실수형(float). default = 0.5\n",
    "        - 엘라스틱 넷의 혼합모수로 0에서 1사이의 값을 입력한다.\n",
    "        - l1_ratio 가 0 이면 위의 수식에서 확인할 수 있듯 알고리즘이 Ridge 회귀와 동일해지고\n",
    "        - l1_ratio 가 1 이면 알고리즘이 LASSO와 동일해진다.\n",
    "    - max_iter : \n",
    "        - 정수형(int), default = None\n",
    "        - Ridge, LASSO와 마찬가지로 iterative한 모형이기 때문에\n",
    "        - 최적 계수 근사치 수렴을 위한 최대 반복수를 지정한다.\n",
    "​    -  fit_intercept​​ :\n",
    "        - 논리형(bool). default = True\n",
    "        - 사용할 모형에 절편을 추가할지 말지 결정한다.      \n",
    "    - normalize​​ : \n",
    "        - 논리형(bool). default = False\n",
    "        - 인자에 True 를 입력 시 추후 fitting 하는 X의 관측들에 대해 각 변수마다 평균값을 빼고 L2 norm으로 나누어 정규화를 실행한다\n",
    "        - fit_intercept 옵션이 False라면 작동하지 않는다. \n",
    "\n",
    "- attributes\n",
    "    - coef_ : \n",
    "        - numpy 의 array 형 반환    \n",
    "        - fitting 된 모형의 계수 추정치 벡터를 넘파이 배열 형태로 반환한다. \n",
    "    - intercept_ :\n",
    "        - float 혹은 array \n",
    "        - fitting 된 모형의 절편 추정치를 실수로 반환한다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomforest Regressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameters\n",
    "    - n_estimators: 랜덤포레스트를 구성하는 결정나무의 개수로 기본값은 100입니다. \n",
    "    - criterion: \n",
    "        - 결정 나무의 노드를 분지할 때 사용하는 불순도 측정 방식으로, 'mse', ',mae' 중 하나로 입력합니다.\n",
    "        - 최근 버전(1.2)에서는 각각 \"squared_error\"와 \"absolute_error\"로 입력합니다. \n",
    "    - max_depth: \n",
    "        - 결정 나무의 최대 깊이입니다. \n",
    "        -  None으로 입력하며 잎 노드가 완전 순수해지거나 모든 잎 노드에 min_samples_split_samples보다 적은 수의 샘플을 포함할 때까지 결정 나무를 학습시킵니다.\n",
    "    - max_features:\n",
    "        - 결정 나무를 분지할 때 고려하는 특징 수(int) 혹은 비율(float)입니다.\n",
    "        - 기본값은 sqrt로 특징 개수에 루트를 씌운 값입니다. \n",
    "    - max_samples\n",
    "        - 각 결정 나무를 학습하는 데 사용할 샘플의 개수(int) 혹은 비율(float)입니다. \n",
    "    - max_leaf_nodes\n",
    "        - default : None\n",
    "        - 리프노드의 최대 갯수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightgbm.LGBMRegressor\n",
    "- classlightgbm.LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=-1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=None, importance_type='split', **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameters\n",
    "   - max_depth :\n",
    "      - Tree의 최대 깊이를 말합니다. \n",
    "   - min_data_in_leaf : \n",
    "      - Leaf가 가지고 있는 최소한의 레코드 수입니다.\n",
    "      - 디폴트값은 20으로 최적 값\n",
    "   - feature_fraction\n",
    "      - Boosting이 랜덤 포레스트일 경우 사용합니다\n",
    "      - 0.8이면 Light GBM이 Tree를 만들 때 매번 각각의 iteration 반복에서 파라미터 중에서 80%를 랜덤하게 선택하는 것을 의미합니다.\n",
    "   - bagging_fraction \n",
    "      - 매번 iteration을 돌 때 사용되는 데이터의 일부를 선택하는데 트레이닝 속도를 높이고 과적합을 방지할 때 주로 사용됩니다.\n",
    "   - lambda : \n",
    "      - lambda 값은 regularization 정규화를 합니다.\n",
    "      -  일반적인 값의 범위는 0 에서 1 사이입니다.\n",
    "   - max_cat_group : \n",
    "      - 카테고리 수가 클 때, 과적합을 방지하는 분기 포인트를 찾습니다.\n",
    "      - 그래서 Light GBM 알고리즘이 카테고리 그룹을 max_cat_group 그룹으로 합치고 그룹 경계선에서 분기 포인트를 찾습니다.\n",
    "      -  디폴트 값은 64 입니다.\n",
    "   - Task : \n",
    "      - 데이터에 대해서 수행하고자 하는 임무를 구체화합니다. \n",
    "      - train 트레이닝일 수도 있고 predict 예측일 수도 있습니다.\n",
    "   - application :\n",
    "      - 가장 중요한 파라미터로, 모델의 어플리케이션을 정하는데 이것이 regression 회귀분석 문제인지 또는 classification 분류 문제인지를 정합니다. \n",
    "      - Light GBM에서 디폴트는 regression 회귀분석 모델입니다.\n",
    "   - boosting : \n",
    "      - 실행하고자 하는 알고리즘 타입을 정의합니다. 디폴트값은 gdbt 입니다.\n",
    "      - gdbt : Traditional Gradient Boosting Decision Tree\n",
    "      - rf : Random Forest\n",
    "      - dart : Dropouts meet Multiple Additive Regression Trees\n",
    "      - goss : Gradient-based One-Side Sampling\n",
    "   - learning_rate : \n",
    "      - 최종 결과에 대한 각각의 Tree에 영향을 미치는 변수입니다.\n",
    "      -  GBM은 초기의 추정값에서 시작하여 각각의Tree 결과를 사용하여 추정값을 업데이트 합니다. \n",
    "      - 학습 파라미터는 이러한 추정에서 발생하는 변화의 크기를 컨트롤합니다. 일반적인 값은 0.1, 0.001, 0.003 등등이 있습니다.\n",
    "   - num_boost_round : \n",
    "      - boosting iteration 수로 일반적으로 100 이상입니다.\n",
    "   - num_leaves : \n",
    "      - 전체 Tree의 leave 수 이고, 디폴트값은 31입니다.\n",
    "   - device : \n",
    "      - 디폴트 값은 cpu 인데 gpu로 변경할 수도 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientBoostingRegressor\n",
    "- class sklearn.ensemble.GradientBoostingRegressor(*, loss='squared_error', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Parameters:\n",
    "   \n",
    "    - loss: \n",
    "        - 최적화할 손실함수를 선택하는 것이다.'ls'는 최소 제곱(Least Square) 회귀를 나타낸다.\n",
    "        - 'lad'(최소 절대 편차)는 입력 변수의 순서 정보만을 기반으로하는 매우 강력한 손실 함수이다.\n",
    "        - 'huber'는이 둘의 조합이다.\n",
    "        - 'quantile'은 분위수 회귀를 허용한다. (default=’ls’)\n",
    "\n",
    "    - learning_rate:\n",
    "        - 각 트리의 기여도를 나타낸다. (default=0.1)\n",
    "    - n_estimators:\n",
    "        -  부스팅 단계의 수를 나타낸다. (default=100)\n",
    "    - subsample:    \n",
    "        - 샘플비율을 나타낸다.\n",
    "        - 1보다 작으면 확률적 그라데이션 부스팅이 발생한다. (default=1.0)\n",
    "    - criterion:\n",
    "        - 분할하는데의 기준을 선택하는 것이다.\n",
    "        -  'friedman_mse', 'mse', 'mae'가 존재하며 일반적으로 defalut값인 friedman_mse가 기능이 좋다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB regressor\n",
    "- 약한 분류기를 세트로 묶어서 정확도를 예측하는 기법이다.\n",
    "- 욕심쟁이(Greedy Algorithm)을 사용하여 분류기를 발견하고 분산처리를 사용하여 빠른 속도로 적합한 비중 파라미터를 찾는 알고리즘이다.\n",
    "- boostin 알고리즘이 기본원리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- parameters\n",
    "    - eta: \n",
    "        - learning rate와 같다. \n",
    "        - 트리에 가지가 많을 수록 과적합하기 쉽다. 매 부스팅 스탭마다 weight를 주어 부스팅 과정에 과적합이 일어나지 않도록 한다.\n",
    "    - gamma: \n",
    "        - 정보흭득(information Gain)에서 -r로 표현한 바 있다.\n",
    "        - 이것이 커지면, 트리 깊이가 줄어들어 보수적인 모델이 된다. ( 디폴트는 0 )\n",
    "    - max_depth : \n",
    "        - 한 트리의 maxium depth.\n",
    "        - 숫자가를 키울수록 보델의 복잡도가 커진다. 과적합 하기 쉽다. \n",
    "        - 디폴트는 6, 이 때 리프노트의 개수는 최대 2^6 = 64개이다.\n",
    "    - lambda (L2 reg-form) : \n",
    "        - L2 Regularization Form에 달리는 weights이다. 숫자가 클수록 보수적인 모델이 된다.\n",
    "    - alpha(L1 reg-form) :\n",
    "        - L1 Regularization Form에 달리는 weights이다. 숫자가 클수록 보수적인 모델이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.linear_model.LogisticRegression\n",
    "* class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "* prarmeters\n",
    "    - penalty :{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’\n",
    "    - C :Inverse of regularization strength , float, default=1.0\n",
    "    - max_iter : 최대 몇번 학습 할건지(에포크), int, default=100\n",
    "    - multi_class{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’\n",
    "        - 'auto' : 알아서 계산해준다\n",
    "        - ‘ovr’: 이진분류\n",
    "        - ‘multinomial’는 solver=’liblinear’일때 불가능.\n",
    "        - ‘auto’ ---> ‘ovr’ : 데이터가 이진 분류 혹은  solver=’liblinear’일때\n",
    "        - ‘auto’ ---> ‘multinomial’: 데이터가 다중 분류 혹은 solver가 나머지거 일때\n",
    "    \n",
    "    - solver : {‘lbfgs’, ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "        - For small datasets, ‘liblinear’ is a good choice\n",
    "        - ‘sag’ and ‘saga’ are faster for large ones;\n",
    "        - \n",
    "* multi- classifier : 레이블이 여러개 값 일때\n",
    "    - 시그모이드에 대입하는 것이 아니라, softmax function에 대입해야 한다\n",
    "    - 엔트로피를 카테고리컬 엔트로피를 구해준다(one-hot encoding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
